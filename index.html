<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision Guide AI - Prototype</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; background: #121212; color: white; }
        #video-container { position: relative; display: inline-block; margin-top: 20px; }
        video { border: 5px solid #00ff00; border-radius: 10px; width: 90%; max-width: 600px; }
        .btn-mic { background: #ff4b2b; color: white; padding: 20px; border-radius: 50%; border: none; cursor: pointer; font-size: 20px; margin-top: 20px; }
        #status { font-size: 1.2rem; color: #00ff00; margin-top: 10px; }
    </style>
</head>
<body>

    <h1>Vision Guide AI Prototype</h1>
    <p>Voice-based Navigation & Object Detection</p>

    <div id="video-container">
        <video id="webcam" autoplay muted></video>
    </div>

    <br>
    <button class="btn-mic" onclick="startVoice()">ðŸŽ¤ Tap to Speak</button>
    <div id="status">System Ready...</div>

    <script>
        const video = document.getElementById('webcam');
        const statusText = document.getElementById('status');
        let model;

        // 1. Setup Webcam
        async function setupWebcam() {
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;
        }

        // 2. Load AI Model (Object Detection)
        async function loadModel() {
            statusText.innerText = "Loading AI Models...";
            model = await cocoSsd.load();
            statusText.innerText = "AI Models Loaded! Detecting...";
            detectObjects();
        }

        // 3. Detect Objects & Speak Alerts
        async function detectObjects() {
            const predictions = await model.detect(video);
            predictions.forEach(prediction => {
                if (prediction.score > 0.6) { // 60% confidence
                    console.log(prediction.class);
                    // If object is very close (simple logic)
                    if(prediction.class === 'person' || prediction.class === 'chair') {
                        speak(`Alert: ${prediction.class} detected ahead`);
                    }
                }
            });
            setTimeout(detectObjects, 3000); // Check every 3 seconds to avoid noise
        }

        // 4. Voice Assistant Logic
        function startVoice() {
            const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            recognition.lang = 'en-US';
            statusText.innerText = "Listening...";

            recognition.onresult = (event) => {
                const command = event.results[0][0].transcript.toLowerCase();
                statusText.innerText = "You said: " + command;
                handleCommand(command);
            };
            recognition.start();
        }

        function handleCommand(cmd) {
            if (cmd.includes('hospital')) {
                speak("Searching for nearby hospitals. Please walk straight for 200 meters.");
            } else if (cmd.includes('where am i')) {
                speak("You are currently at Main Road, Coimbatore.");
            } else {
                speak("I heard you, but I am still learning that command.");
            }
        }

        function speak(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            window.speechSynthesis.speak(utterance);
        }

        // Start everything
        setupWebcam();
        loadModel();
    </script>
</body>
</html>
